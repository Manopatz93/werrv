{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5c3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.1.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\python\\envs\\dab300\\lib\\site-packages (from google-search-results) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\envs\\dab300\\lib\\site-packages (from requests->google-search-results) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\envs\\dab300\\lib\\site-packages (from requests->google-search-results) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\envs\\dab300\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\envs\\dab300\\lib\\site-packages (from requests->google-search-results) (2022.9.24)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py): started\n",
      "  Building wheel for google-search-results (setup.py): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.1-py3-none-any.whl size=25775 sha256=dd1b1f1736a66b8df059b73414f99d7474c65df5fe7784178d41590fd274d924\n",
      "  Stored in directory: c:\\users\\91887\\appdata\\local\\pip\\cache\\wheels\\82\\a3\\c5\\364155118f298722dff2f79ae4dd7c91e92b433ad36d6f7e0e\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.1\n"
     ]
    }
   ],
   "source": [
    "# pip install google-api-python-client\n",
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29905647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "  \"q\": \"Coffee\",\n",
    "  \"location\": \"Austin, Texas, United States\",\n",
    "  \"hl\": \"en\",\n",
    "  \"gl\": \"us\",\n",
    "  \"google_domain\": \"google.com\",\n",
    "  \"api_key\": \"3787f2ecc9d73991d3cee076211dc3f5a02f6db87863599a9b7855716c760524\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f00a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "search = GoogleSearch({\"q\": \"coffee\", \"location\": \"Austin,Texas\", \"api_key\": \"secretKey\"})\n",
    "result = search.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1957c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://serpapi.com/search.json?q=Coffee&location=Austin,+Texas,+United+States&hl=en&gl=us&google_domain=google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa9d2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c40cd36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2184\\2064229405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_dataforseo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def search_dataforseo(query, api_key):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Api-Key\": api_key\n",
    "    }\n",
    "\n",
    "    url = \"https://api.dataforseo.com/v3/keywords_data/serp/google/task_post\"\n",
    "\n",
    "    payload = {\n",
    "        \"data\": {\n",
    "            \"keywords\": [query],\n",
    "            \"locations\": [\"US\"],\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    search_results = response.json()\n",
    "    return search_results\n",
    "\n",
    "query = \"google keyword search api\"\n",
    "api_key = \"917100dfd344bf83\"\n",
    "search_results = search_dataforseo(query, api_key)\n",
    "\n",
    "for result in search_results[\"results\"][0][\"data\"]:\n",
    "    print(result[\"title\"])\n",
    "    print(result[\"url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c96f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: API request failed with status code 401\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def search_dataforseo(query, api_key):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Api-Key\": api_key\n",
    "    }\n",
    "\n",
    "    url = \"https://api.dataforseo.com/v3/keywords_data/serp/google/task_post\"\n",
    "\n",
    "    payload = {\n",
    "        \"data\": {\n",
    "            \"keywords\": [query],\n",
    "            \"locations\": [\"US\"],\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "    search_results = response.json()\n",
    "    if \"results\" not in search_results:\n",
    "        raise Exception(\"No results found in the API response\")\n",
    "\n",
    "    return search_results\n",
    "\n",
    "query = \"google keyword search api\"\n",
    "api_key = \"917100dfd344bf83\"\n",
    "\n",
    "try:\n",
    "    search_results = search_dataforseo(query, api_key)\n",
    "    for result in search_results[\"results\"][0][\"data\"]:\n",
    "        print(result[\"title\"])\n",
    "        print(result[\"url\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1282ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE NEXT IS USING BACKLINKS ANALYSIS OF OUR WEBSITE AND COMPETITORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41d37ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.facebook.com/Werrv.ca/', 'https://twitter.com/werrv_tweets', 'https://www.instagram.com/werrv.ca/', 'https://www.pinterest.com/werrv/', 'https://www.youtube.com/Werrv', 'https://www.tiktok.com/@werrvinc', 'https://werrv.com/collections/trailering', 'https://werrv.com/collections/boat-outfitting', 'https://werrv.com/collections/watersports', 'https://werrv.com/collections/camping', 'https://werrv.com/collections/marine-navigation-instruments', 'https://werrv.com/collections/winterizing', 'https://www.shopify.com?utm_campaign=poweredby&utm_medium=shopify&utm_source=onlinestore', 'https://www.facebook.com/Werrv.ca/', 'https://twitter.com/werrv_tweets', 'https://www.instagram.com/werrv.ca/', 'https://www.pinterest.com/werrv/', 'https://www.youtube.com/Werrv', 'https://www.tiktok.com/@werrvinc', 'https://www.shopify.com?utm_campaign=poweredby&utm_medium=shopify&utm_source=onlinestore']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract backlinks from a URL\n",
    "def get_backlinks(url):\n",
    "    backlinks = []\n",
    "    source_code = requests.get(url).text\n",
    "    soup = BeautifulSoup(source_code, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and href.startswith(\"http\"):\n",
    "            backlinks.append(href)\n",
    "    return backlinks\n",
    "\n",
    "# The URL you want to analyze\n",
    "url = \"https://werrv.ca/\"\n",
    "\n",
    "# Get the backlinks from the URL\n",
    "backlinks = get_backlinks(url)\n",
    "\n",
    "# Print the list of backlinks\n",
    "print(backlinks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d663f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE ABOVE CODE WE SCRAPPED ALL THE BACKLINKS FROM THE WERRV WEBSITE FOR FUTURE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30edf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.amazon.ca/gp/help/customer/display.html/ref=footer_cou?ie=UTF8&nodeId=1040616', 'https://www.amazon.ca/gp/help/customer/display.html/ref=footer_privacy?ie=UTF8&nodeId=502584']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract backlinks from a URL\n",
    "def get_backlinks(url):\n",
    "    backlinks = []\n",
    "    source_code = requests.get(url).text\n",
    "    soup = BeautifulSoup(source_code, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and href.startswith(\"http\"):\n",
    "            backlinks.append(href)\n",
    "    return backlinks\n",
    "\n",
    "# The URL you want to analyze\n",
    "url = \"https://www.amazon.ca/Command-NCTP100-Monitoring-Bluetooth-Enabled-Bluetooth/dp/B0848SHHSN\"\n",
    "\n",
    "# Get the backlinks from the URL\n",
    "backlinks = get_backlinks(url)\n",
    "\n",
    "# Print the list of backlinks\n",
    "print(backlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e00b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract backlinks from a URL\n",
    "def get_backlinks(url):\n",
    "    backlinks = []\n",
    "    source_code = requests.get(url).text\n",
    "    soup = BeautifulSoup(source_code, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and href.startswith(\"http\"):\n",
    "            backlinks.append(href)\n",
    "    return backlinks\n",
    "\n",
    "# The URL you want to analyze\n",
    "url = \"https://www.bestbuy.ca/en-ca/product/in-command-tire-pressure-monitoring-system-4-sensor-amp-repeater-package/14665350\"\n",
    "\n",
    "# Get the backlinks from the URL\n",
    "backlinks = get_backlinks(url)\n",
    "\n",
    "# Print the list of backlinks\n",
    "print(backlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23eff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding a new topic of changing a meta data and title automatically using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The URL you want to optimize\n",
    "url = \"https://werrv.ca/\"\n",
    "\n",
    "# Get the HTML source code for the URL\n",
    "source_code = requests.get(url).text\n",
    "soup = BeautifulSoup(source_code, \"html.parser\")\n",
    "\n",
    "# Update the title tag\n",
    "title_tag = soup.find(\"title\")\n",
    "title_tag.string = \"New title for the page\"\n",
    "\n",
    "# Update the meta description\n",
    "meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "meta_description[\"content\"] = \"New description for the page\"\n",
    "\n",
    "# Save the updated HTML source code\n",
    "with open(\"optimized.html\", \"w\") as f:\n",
    "    f.write(str(soup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ff2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a complex methodoligy in which we are going to compare the image quality of the werrv and the competitors website\n",
    "# we can use someother machine learning model\n",
    "# but for scrapping such task we need proper code for every website and number of images\n",
    "# we practically i dont know how this is possible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "def get_images(url):\n",
    "    images = []\n",
    "    source_code = requests.get(url).text\n",
    "    soup = BeautifulSoup(source_code, \"html.parser\")\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and src.startswith(\"http\"):\n",
    "            images.append(src)\n",
    "    return images\n",
    "\n",
    "def compare_image_quality(url1, url2):\n",
    "    images1 = get_images(url1)\n",
    "    images2 = get_images(url2)\n",
    "    if len(images1) != len(images2):\n",
    "        return \"Number of images is different between the two URLs\"\n",
    "\n",
    "    for i in range(len(images1)):\n",
    "        img1 = Image.open(requests.get(images1[i], stream=True).raw)\n",
    "        img2 = Image.open(requests.get(images2[i], stream=True).raw)\n",
    "        if img1.size != img2.size:\n",
    "            return \"Image size is different for image {}\".format(i + 1)\n",
    "        if img1.format != img2.format:\n",
    "            return \"Image format is different for image {}\".format(i + 1)\n",
    "\n",
    "    return \"Image quality is similar between the two URLs\"\n",
    "\n",
    "# The URLs you want to compare\n",
    "url1 = \"https://werrv.ca/\"\n",
    "url2 = \"https://www.example.com/\"\n",
    "\n",
    "# Compare the image quality of the two URLs\n",
    "result = compare_image_quality(url1, url2)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b84ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.seoclarity.net/blog/reporting-on-google-flights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
